\chapter{Einleitung}
\label{ch:einleitung}
Die Idee für eine Maschine, die anhand eingegebener Daten selbstständig Entscheidungen treffen kann, ist schon sehr alt. Praktischen Ansätze für künstliche neuronale Netze gibt es seit einigen Jahrzehnten. Der weitreichende Einsatz derartiger Algorithmen findet erst in neuerer Zeit statt. Viele Erfindungen, die vor 30 oder 50 Jahren in Filmen und Serien Science-Fiction darstellten, sind inzwischen nicht nur Realität, sondern auch alltagstauglich. Zu den wichtigsten Beispielen zählen verbale Schnittstellen an Computersystemen und in Armbanduhren, autonome Fahrzeuge etwa in Gestalt von Parkassistenten und verschiedene Verfahren zur biometrischen Identitätsprüfung.

Andere schnelle technologische Fortschritte aus der Vergangenheit haben nicht immer nur die Lebensqualität der beteiligten Personen erhöht. Sie stellten durch Missbrauch gelegentlich sogar Gefahren dar. So wie das Internet auch zur Verbreitung von Falschinformationen und die sichere Verschlüsselung von gespeicherten Daten auch zur Verschleierung von Straftaten genutzt werden kann, ist die Generierung von täuschend echten Bildern unter Umständen geeignet, persönlichen, finanziellen oder anders gearteten Schaden zu verursachen.

Weiterhin existieren bei der Auswahl der Trainingsdaten für künstliche neuronale Netze rechtliche Grenzen. Bilddaten sind in mehr als ausreichenden Mengen vorhanden, berücksichtigen aber zum Beispiel nicht immer das Recht am eigenen Bild. Für diese Bachelorarbeit sind die Anforderungen an die Bildqualität außerdem sehr hoch, da möglichst auch Texturen und Lichtreflexionen erlernt werden sollen. Modellgrafiken mit geeigneten Material- und Beleuchtungseigenschaften gibt es zwar auch, aber nur in weitaus geringeren Mengen. Für solche Fremdarbeiten, die sehr arbeitsaufwendig sind, wäre auch die Klärung der Nutzungsrechte notwendig geworden. Für Trainingsergebnisse, die das mentale Modell einer breiten Nutzergemeinschaft reflektieren, sind grundsätzlich auch Daten aus möglichst vielen verschiedenen Quellen erforderlich.

Es ist deshalb eine Kombination aus Trainingsdaten, die aus ausgewählten Benutzereingaben bestehen, und solchen, die bestimmte Qualitätseigenschaften erfüllen und in beliebiger Menge erstellt werden können, entstanden. Ein künstliches neuronales Netz soll Skizzen aus dem Dataset des ``Quick, Draw!''-Minigames\footnote{\href{https://quickdraw.withgoogle.com/}{https://quickdraw.withgoogle.com/}} in hochwertig gestaltete Figuren umwandeln.

\section{Ziel der Arbeit}
\label{sec:ziel}
Für diese Arbeit habe ich mir zum Ziel gesetzt, anhand wissenschaftlicher Literatur die Grundlagen und Methoden des maschinellen Lernens zu erarbeiten. Im Rahmen des Themas dieser Abschlussarbeit habe ich mit unterschiedlichen künstlichen neuronalen Netzen experimentiert, um Bilder zu generieren.

Im Zuge der Ausarbeitung des Konzeptes waren zunächst verschiedene Rahmenbedingungen zu bewerten. Trainingsdaten müssen in großen Mengen verfügbar sein. Urheberrechts- oder Lizenzfragen sollten so selten wie möglich auftreten. Weiterhin sollten die Eingabebilder eine gewisse Homogenität aufweisen, um ein Modell auf die Aspekte Textur und Shading trainieren zu können. Meine Recherche verschiedener Methoden zur Generierung von Bildern durch künstliche neuronale Netze ist im \hyperref[sec:related]{nächsten Abschnitt zu den bisherigen Arbeiten} näher beschrieben.

Die Entscheidung fiel schließlich auf ein Machine-Learning-Modell namens Image-To-Image-Translation \cite{isola2018imagetoimage} oder kurz Pix2Pix, das unter anderem die Ergebnisse der Canny-Edge-Detection \cite{canny1986edge} eines Bildes, also nur die Umrisse des abgebildeten Objekts, in das Originalbild zurückübersetzen kann. In meinen Experimenten habe ich das Modell erfolgreich darauf trainiert, handgezeichnete Skizzen in fotorealistische Abbildungen von Alltagsgegenständen zu übersetzen. Die Grundlagen und Bausteine des Algorithmus sind in \hyperref[ch:model]{Kapitel \ref{ch:model}} und der Algorithmus selbst und seine Implementierung in Python im Abschnitt zu \hyperref[sec:pix2pix]{Image-To-Image-Translation} in \hyperref[ch:impl]{Kapitel \ref{ch:impl}} beschrieben.

Das Quick-Draw-Dataset ist eine umfangreiche Sammlung handgezeichneter Skizzen von Gegenständen. Ziel des Minigames von Google ist es, ein vorgegebenes Motiv innerhalb 20 Sekunden mithilfe einer Maus, eines Touchpads oder einem vergleichbaren Eingabegerät so zu zeichnen, dass ein zeitgleich aktiver Algorithmus die Zeichnung klassifizieren kann. Das Dataset besteht aus den erstellten Zeichnungen, ist nach Motiven sortiert und ebenfalls kostenlos verfügbar. Somit ist es für meine Experimente bestmöglich geeignet. Die Verwendung des Quick-Draw-Datasets ist in \hyperref[ch:conduct]{Kapitel \ref{ch:conduct}} beschrieben.

Es gibt verschiedene denkbare praktische Anwendungsfälle für diese automatische Übersetzung. Vergleichbar mit Handschrifterkennung oder der Erkennung primitiver geometrischer Formen durch Geräte mit Stift- oder Fingereingabe kann die Klassifizierung von handgezeichneten Alltagsgegenständen etwa durch Smartboards die Illustration von Alltagssituationen erleichtern. In meinen Experimenten habe ich Skizzen von Autos in fotorealistische Abbildungen übersetzt. Damit ist die Darstellung einer Situation im Straßenverkehr vorstellbar, wie sie in einer Fahrschule vorteilhaft sein könnte. Ein weiterer konkreter Anwendungsfall ist der elektronisch gestützte Entwurf einer Inneneinrichtung mit verschiedenen Möbelstücken.

Es ist möglich, die Zahl der vorhandenen Eingabebilder durch Data Augmentation zu erhöhen \cite{chollet2021deep}. Dabei werden die Bilder gespiegelt, gedreht, skaliert oder auf andere Weise verändert oder verfälscht. Es ist außerdem denkbar, die Eingabedaten selbst zu erstellen. Die Aufgabenstellung gibt einen Zusammenhang mit prozeduralen Shadern vor. Ich habe ich mich deshalb dazu entschieden, einen wesentlichen Teil der Eingabebilder selbst zu entwerfen und automatisiert zu erstellen. Es sind Bilder verschiedener Alltagsgegenstände entstanden, die sich in der Form unterscheiden, deren Texturen und Lichtverhältnisse sich aber gleichen. Dadurch war es mir möglich, das Modell auf die Übersetzung der handgezeichneten Skizzen in passende, fotorealistische Bilder zu trainieren. Das Erstellen der 3D-Modelle sowie die Automatisierung ist ebenfalls in \hyperref[ch:conduct]{Kapitel \ref{ch:conduct}} beschrieben.

Eingabedaten für das Training der Image-To-Image-Translation sind also einerseits handgezeichnete Skizzen und andererseits fotorealistische Abbildungen alltäglicher Gegenstände. In der Literatur wird häufig das Bild, das übersetzt werden soll, als Input bezeichnet. Das Bild, in das übersetzt werden soll, wird Ground Truth oder Target genannt \cite{chollet2021deep}. Ich verwende in diesem Dokument dieselben Bezeichnungen oder die deutschen Übersetzungen Eingabebild für Input und Zielbild für Target.

Im Verlauf der Experimente habe ich den Trainingsfortschritt und -erfolg verschiedener Eingabebilder und Algorithmen betrachtet. Die interessantesten Größen beim maschinellen Lernen sind die Menge der Eingabedaten, die Anzahl der Trainingsdurchläufe und im Zusammenhang damit die Dauer des Trainings. Das tatsächliche Trainingsergebnis habe ich in Augenschein genommen und subjektiv für zufriedenstellend oder nicht zufriedenstellend befunden.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.3\textwidth]{bilder/0.jpg}
	\includegraphics[width=0.3\textwidth]{bilder/1.jpg}
	\includegraphics[width=0.3\textwidth]{bilder/15.jpg}
	\caption[Verschiedene Trainingsschritte]{Das Modell generiert das Ergebnis in mehreren Tausend Schritten aus zufälligem Rauschen.}
	\label{fig:trainingsteps}
\end{figure}

\section{Bisherige Arbeiten}
\label{sec:related}
Künstliche neuronale Netze finden erst seit wenigen Jahren breite Aufmerksamkeit, seit auch Heimcomputer in der Lage sind, die hohe Anzahl der erforderlichen Rechenoperationen in annehmbarer Zeit auszuführen. Seitdem sind wenige englischsprachige Einführungen in die Thematik entstanden. Ein häufig genanntes Buch ist ``Deep Learning'', das online kostenfrei zugänglich ist \cite{goodfellow2016deeplearning}. Ebenfalls online und kostenfrei ist das Buch ``Dive into Deep Learning'' \cite{zhang2020dive}. Ein weiteres, etwas praxisorientierteres Buch ist ``Deep Learning with Python'' \cite{chollet2021deep}.

Erste Recherchen haben verschiedene Arten und Implementierungen Bilder generierender künstlicher neuronaler Netze herausgestellt. Das Model aus ``DRAW: A Recurrent Neural Network For Image Generation'' \cite{gregor2015draw} kann darauf trainiert werden, handschriftliche Ziffern wie die des MNIST-Datasets \cite{deng2012mnist} zu generieren.

Die Schlüsselerkenntnis in ``A Neural Algorithm of Artistic Style'' \cite{gatys2015nst} ist, dass Inhalt und Stil eines Bildes voneinander getrennt werden können. Das Modell kann zum Beispiel den Stil eines Künstlers auf das Gemälde eines anderen übertragen.

In anderen Ansätzen mit rekurrenten neuronalen Netzen wird jedes Eingabebild in eine Sequenz von Pixeln umgeformt. Anschließend wird das Modell darauf trainiert, teilweise geschwärzte Bilder wieder zu vervollständigen \cite{chen2020generative}, \cite{oord2016pixel}. Es werden auch Bilder generiert und die Fähigkeit des jeweiligen Modells, die Darstellung von Bildern zu lernen, untersucht. Die Ergebnisse werden mit denen anderer Modelle verglichen. Ein weiterer, ebenfalls rekurrenter Algorithmus generiert Bilder aus textuellen Beschreibungen \cite{ramesh2021zeroshot}.

Besondere Aufmerksamkeit habe ich den Generative Adversarial Networks \cite{goodfellow2014generative} gegeben. Die Ergebnisse dieser Familie von künstlichen neuronalen Netzen sind teilweise erstaunlich gut, und es existieren bereits einige Beispielanwendungen und Implementierungen. Je nach Anwendung des Models variiert die Qualität der Resultate.

In ``Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks'' \cite{radford2016unsupervised} werden ebenfalls
realitätsnahe Ergebnisse erzielt. Anders als bei den bisher erwähnten Arbeiten
wird Unsupervised Learning verwendet, um das künstliche neuronale Netz zu trainieren.

Die vielseitigste Generierung von Bildern bietet Image-To-Image-Translation with Conditional Adversarial Networks \cite{isola2018imagetoimage}. In der Arbeit werden Labels in Fotos zurückübersetzt, Schwarz-Weiß-Bilder koloriert und weitere Anwendungsbeispiele gezeigt. Vor allem die Übersetzung ``Kanten nach Foto'' (``Edges to Photo'') ist für mein Vorhaben relevant. Das Dokument enthält dafür mehrere Beispiele. Der Algorithmus setzt auf Generative Adversarial Networks \cite{goodfellow2014generative} und Conditional Generative Adversarial Networks \cite{mirza2014conditional} auf.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.6\textwidth]{bilder/table1small.png}
	\includegraphics[width=0.6\textwidth]{bilder/table2small.png}
	\includegraphics[width=0.6\textwidth]{bilder/table3small.png}

	\includegraphics[width=0.6\textwidth]{bilder/candle1small.png}
	\includegraphics[width=0.6\textwidth]{bilder/candle2small.png}
	\includegraphics[width=0.6\textwidth]{bilder/candle3small.png}
	\caption[Eigene Beispiele]{Meine Ergebnisse zeigen, dass aus einer Handzeichnung ein fotorealistisches Bild generiert werden kann. Die generierten Bilder nehmen ungefähr die Proportionen der Handzeichnungen an, erstellen realistische Darstellungen der Gegengenstände und fügen Texturen und Lichteffekte hinzu.}
	\label{fig:myexamples}
\end{figure}

Eine für die Vorbereitung meiner Experimente erforderliche Software ist Blender. Das Programm ist kostenlos verfügbar und ermöglicht die Modellierung von 3D-Szenen. Objekte können aus vorgegebenen Würfeln, Zylindern und vielen weiteren Körpern und auch Flächen erstellt, skaliert und mit verschiedenen Werkzeugen verformt werden. Es stehen Lichtquellen zur Verfügung, und eine Szene kann als Bilddatei gespeichert werden. Zur Verwendung der Software gibt es eine große Auswahl an Literatur.
